# -*- coding: utf-8 -*-
"""ehealth_lab_4_code.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1SDuCiDBC56oPDoxd1HlnrgqGx6PxjvSP

**E-HEALTH - LABORATORY 4**

# **DOWNLOAD THE DATASET**

In this laboratory, we will use the data that are available at link https://drive.google.com/file/d/1Bo5rCQYoeoCzKrMi8rMpVsjqfyuqDMVH/view?usp=sharing

We download the folder and add the data to the COLAB Notebook.
"""

# Commented out IPython magic to ensure Python compatibility.
# %pip install googledrivedownloader
from googledrivedownloader import download_file_from_google_drive
import zipfile

download_file_from_google_drive(file_id='1Bo5rCQYoeoCzKrMi8rMpVsjqfyuqDMVH',
                                dest_path='./ehealth_lab_3.zip',
                                unzip=False,
                                overwrite=True)

with zipfile.ZipFile("./ehealth_lab_3.zip","r") as zip_ref:
    zip_ref.extractall("./ehealth_lab_3")

path_to_dataset_folder = "./ehealth_lab_3/"

"""We install and import `mne` and other useful Python packages."""

# Commented out IPython magic to ensure Python compatibility.
# %pip install mne
import mne
import os
import scipy
import numpy as np
import matplotlib.pyplot as plt
from scipy.stats import entropy

"""We define a method to plot a group of channels."""

def plot_signal_channels(times: np.ndarray,
                         signal_channels: np.ndarray,
                         channel_names: list,
                         amplitude: float = 1.0,
                         title: str = None):

    _, axes  = plt.subplots()

    ax = axes
    ax.set_facecolor('lightyellow')
    sub_ax = None

    channel_num = len(channel_names)

    shift = 2 * amplitude

    for index in range(channel_num):

        ax.plot(times, signal_channels[index] - (index + 1) * shift, color='k', linewidth=0.25)

    ax.set_yticks([- i * shift for i in range(1, channel_num + 1)], channel_names)

    ax.grid('on', which='both')
    ax.set_ylabel('Channel')
    ax.set_xlim([times[0], times[-1]])

    ax.set_ylim([-(channel_num + 1) * shift, shift])

    if title is not None:
        plt.title(title)

    ax.set_xlabel('Time [s]')

"""Today, we will work with a dataset of EEG signals from PhysioNet, a free-available repository of medical data, managed by MIT.

The full dataset can be found at https://physionet.org/content/eegmmidb/1.0.0/.

We consider a subset of the original data, organized in subfolders.
"""

os.listdir(path_to_dataset_folder)

"""Each subfolder is associated with a patient. We focus on patient S001.



"""

patient_name = 'S001'

path_to_patient_folder = path_to_dataset_folder + patient_name + '/' # The folder of the patient that we want to study

os.listdir(path_to_patient_folder)

"""We load one of the EEGs of the patient."""

file_name = 'S001R01.edf' # The file that we want to study
raw_data = mne.io.read_raw_edf(path_to_patient_folder +  file_name)

"""Medical information can be found in the `mne.io.Raw.annotations` attribute."""

for annotation in raw_data.annotations:
  print(annotation)

"""Signal information are contained into the `mne.io.Raw.info` attribute."""

raw_data.info

"""We save the sampling frequency, the sampling period, the number of channels, and the channel names into specific variables."""

sampling_frequency = raw_data.info['sfreq'] # Sampling frequency
sampling_period = 1 / raw_data.info['sfreq'] # Sampling period
channel_names = raw_data.info['ch_names'] # List of channel names
channel_size = raw_data.info['nchan'] # Number of channel

"""We save the signal values and the time references."""

signal_values = raw_data.get_data() * 1000  # Unit of measure in millivolt
signal_times = raw_data.times  # Unit of measure in seconds
sample_size = len(signal_times) # Number of samples

"""We consider ten channels and a limited number of samples."""

selected_channel_names = ['P7..', 'P5..', 'P3..', 'P1..', 'Pz..', 'P2..', 'P4..', 'P6..', 'P8..', 'Po7.']
selected_channel_indexes = [channel_names.index(channel) for channel in selected_channel_names]
selected_sample_number = 2**13

observed_samples = np.arange(selected_sample_number)
observed_times = signal_times[:selected_sample_number]
observed_channels = signal_values[selected_channel_indexes, :selected_sample_number]

print(observed_channels.shape)

## The observation includes: 10 channels, each of them containing their first selected_sample_number samples

"""We plot the raw observations, i.e., the observed channels."""

plot_signal_channels(times=observed_times,
                     signal_channels=observed_channels,
                     channel_names=selected_channel_names,
                     amplitude=np.max(np.abs(observed_channels)),
                     title='Channel observation')

"""We assume that these **observed channels** were generated by a limited number of sources and set the goal of reconstructing the **source channels** following an approach based on independent component analysis (ICA).

**INDIPENDENT COMPONENT ANALYSIS (ICA) ON SYNTHETIC DATA**

Before applying the ICA algorithm on the EEG data, we consider an example, where six observed signals are generate from four sources defined as follows.
"""

source_signals = np.array([0.3 + 0.5 * np.sin(0.02 * observed_samples),
                           -0.2 + 0.8 * scipy.signal.square(0.05 * observed_samples),
                           0.2 * np.cos(0.1 * observed_samples),
                           -0.05 + 1.1 * scipy.signal.chirp(observed_samples, f0=0.00001, f1=0.0001, t1=50, method='linear')])

"""We define the mixing matrix and compute the observed signals."""

mixing_matrix = np.array([[0.1, 0.3, 0.3, 1.0],
                          [0.2, 1.0, 0.1, 0.4],
                          [0.5, 0.8, 0.0, 0.6],
                          [1.0, 0.4, 0.2, 0.8],
                          [0.3, 0.1, 0.5, 0.9],
                          [0.9, 0.0, 0.9, 0.1]])

# Matrix Z that would connect source to observation
# number of rows are the #observations and columns are #sources

observation_number, source_number = mixing_matrix.shape

observed_signals = np.dot(mixing_matrix, source_signals)

plot_signal_channels(times=observed_times,
                     signal_channels=observed_signals,
                     channel_names=[str(i) for i in range(observation_number)],
                     amplitude=np.max(np.abs(observed_signals)),
                     title='Observations')

plot_signal_channels(times=observed_times,
                     signal_channels=source_signals,
                     channel_names=[str(i) for i in range(source_number)],
                     amplitude=np.max(np.abs(source_signals)),
                     title='Sources')
# 4 different brain sources. we want to get to this from observation without knowing the Z matrix

"""We need to implement the pre-processing pipeline for ICA. At first, we need to ensure that the observed channels have a null mean."""

def center(channels: np.ndarray):

  channel_means = np.mean(channels, axis=1, keepdims=True)

  channels -= channel_means

  return channels, channel_means

"""Then, we project the observed channels into a new space to ensure that the channels are uncorrelated."""

def decorrelate(channels: np.ndarray, channel_means: np.ndarray):

  centered_channels = channels - channel_means

  cov = centered_channels.dot(centered_channels.T) / np.shape(channels)[1]

  eigenvalues, eigenvectors = np.linalg.eig(cov)

  return eigenvectors.T.dot(centered_channels), eigenvalues, eigenvectors

"""Finally, we need a function to scale the result according to the eigenvalues."""

def scale(channels: np.ndarray, eigenvalues: np.ndarray):

  return np.dot(np.diag(np.power(eigenvalues + 1e-8, -1/2)), channels)

"""We define a unique method to carry out the above steps."""

def preprocess(observations: np.ndarray):

  observations, observation_means = center(observations)

  observations, eigenvalues, eigenvectors = decorrelate(observations, observation_means)

  return scale(observations, eigenvalues)

"""We apply this pre-processing pipeline to the synthetic data we generated."""

whitened_signals = preprocess(observed_signals)

plot_signal_channels(times=observed_times,
                     signal_channels=whitened_signals,
                     channel_names=[str(i) for i in range(observation_number)],
                     amplitude=np.max(np.abs(whitened_signals)),
                     title='Withened observations')

# last two are like that because they are decollerated

"""We can already perceive the signal sources, but decorrelation is not enough to ensure independence! To obtain a better estimate of the signal sources, we implement the FastICA algorithm."""

def fast_ica(whitened_signals: np.ndarray, source_number: int, convergence_threshold=1e-8, max_iteration=1000):

    original_observation_number, sample_number = whitened_signals.shape

    # only signals with variances greater than zero or equal to one if i heard correctly. So the first 4 remain and the last two will not be considered. row 0-3 ok, row 4,5 discarded
    variances = np.var(whitened_signals, axis=1)
    whitened_signals = whitened_signals[variances > 1e-8]

    observation_number, sample_number = whitened_signals.shape

    # Initialize random weights

    W = np.random.rand(source_number, observation_number)

    for source_index in range(source_number):

        w = W[source_index, :].copy().reshape((-1, 1)) # in order to make it a column vector like the notation in slides

        w = w / np.sqrt((w ** 2).sum())

        iteration_index = 0
        convergence_value = 100

        while ((convergence_value > convergence_threshold) & (iteration_index < max_iteration)):

            # Dot product of weight and whitened signal
            ws = np.dot(w.T, whitened_signals)

            # Pass w*s into the first derivative of G
            g1 = np.tanh(ws)

            # Pass w*s into the second derivative of G
            g2 = 1 / (np.cosh(ws) ** 2)

            # Update weights
            new_w = w * np.sum(g2) / sample_number - (np.dot(whitened_signals, g1.T)) / sample_number

            # Decorrelate weights
            for prev_source_index in range(source_index):

              prev_w = W[prev_source_index, :].copy().reshape((-1, 1))

              new_w -= prev_w * np.dot(new_w.T, prev_w)

            #  Normalize
            new_w = new_w / np.sqrt((new_w ** 2).sum())

            # Calculate limit condition
            convergence_value = np.abs(np.abs(np.dot(new_w.T, w)) - 1).item()

            # Update weights
            w = new_w

            # Update counter
            iteration_index += 1

        W[source_index, :] = w.T

    if original_observation_number != observation_number: # the final shape will be consistant now because we had lower number of observations and by this if loop we will add zero columns to equal the #observations that we had in the begining

        zero_col = np.zeros((source_number, original_observation_number  - observation_number))
        W = np.hstack((W, zero_col))

    return W

"""We exploit FastICA to estimate the demixing matrix associated with the synthetic signals."""

W = fast_ica(whitened_signals, source_number, convergence_threshold=1e-8, max_iteration=1000)
estimated_source_signals = np.dot(W, whitened_signals)

plot_signal_channels(times=observed_times,
                     signal_channels=estimated_source_signals,
                     channel_names=[str(i) for i in range(source_number)],
                     amplitude=np.max(np.abs(whitened_signals)),
                     title='Estimated source signals')

plot_signal_channels(times=observed_times,
                     signal_channels=source_signals,
                     channel_names=[str(i) for i in range(source_number)],
                     amplitude=np.max(np.abs(whitened_signals)),
                     title='True source signals')

# now the only thing different is the variance (amplitude)!

"""**INDIPENDENT COMPONENT ANALYSIS (ICA) ON EEG DATA**

We can try to apply the same framework to the EEG data, assuming to know the right number of signal sources.
"""

source_channel_number = 3
observed_channel_number = len(observed_channels)

"""As before, we apply the pre-processing pipeline to have whitened signals."""

whitened_channels = preprocess(observed_channels)

plot_signal_channels(times=observed_times,
                     signal_channels=whitened_channels,
                     channel_names=[str(i) for i in range(observed_channel_number)],
                     amplitude=np.max(np.abs(whitened_channels)),
                     title='Whitened channels')

"""We implement the FastICA algorithm to estimate the sources of the EEG signal."""

W = fast_ica(whitened_channels, source_channel_number, convergence_threshold=1e-10, max_iteration=500)
estimated_source_channels = np.dot(W, whitened_channels)

plot_signal_channels(times=observed_times,
                     signal_channels=estimated_source_channels,
                     channel_names=[str(i) for i in range(source_channel_number)],
                     amplitude=np.max(np.abs(estimated_source_channels)),
                     title='Estimated source channels')

"""**YOUR TURN**

We have seen how to implement the fastICA algorithm to reconstruct the source signals, both in the case of synthetic and real data. On the other hand, it is hard to tell if the proposed approach works well or not from a quantitative point of view.

From a practical perspective, you can associate performance with the normalized mutual information and check if the fastICA algorithm successfully reduces the amount of information shared between the different signal components. This may give an indication of the optimal number of sources for a given scenario. To compute the mutual information for a given multi-dimensional signal, you can modify the following method.
"""

def get_normalized_mi(signals: np.ndarray):

  n_channels, n_samples = signals.shape

  if n_channels == 0 or n_samples == 0:
      return 0.0

  # Using Sturges' rule based on the number of samples is a reasonable default.
  bin_number = int(np.ceil(np.log2(n_samples) + 1))
  bin_number = max(2, bin_number) # Ensure a minimum of 2 bins

  # Discretize each signal independently
  # We use np.histogram_bin_edges to get consistent bins and then np.digitize
  digitized_signals = []
  marginal_entropies = []

  for i in range(n_channels):
      # Calculate bin edges for each signal
      bins = np.histogram_bin_edges(signals[i], bins=bin_number)

      # Digitize the signal into bins
      # `right=True` means the rightmost bin edge is inclusive
      digitized = np.digitize(signals[i], bins, right=True)

      # Handle edge case where all values fall into the last bin
      if np.all(digitized == bin_number):
           digitized[digitized > 0] -= 1 # Shift to the last valid bin index if all are beyond

      # Calculate marginal entropy for the discretized signal
      # Use np.bincount to get counts of digitized values (bin indices)
      # The range of indices will be from 0 to bin_number - 1
      counts = np.bincount(digitized, minlength=bin_number)

      # Calculate probabilities, ignoring bins with zero counts
      prob_values = counts / np.sum(counts)
      prob_values = prob_values[prob_values > 0] # Only consider non-empty bins

      h = entropy(prob_values)
      marginal_entropies.append(h)
      digitized_signals.append(digitized)

  # Calculate joint entropy on the stacked digitized signals
  # Stack the digitized signals vertically to get shape (n_samples, n_channels)
  # This represents each sample as a vector of bin indices across channels
  joint_signal = np.vstack(digitized_signals).T

  # Use np.unique to find unique rows (combinations of bin indices) and their counts
  # This is equivalent to creating a joint histogram without explicitly building the large array
  if joint_signal.dtype != object: # Avoid potential issues with object dtype if all same type
      # Need to view as bytes to use np.unique on rows efficiently
      joint_signal_unique_rows = np.ascontiguousarray(joint_signal).view(np.dtype((np.void, joint_signal.dtype.itemsize * joint_signal.shape[1])))
      _, counts = np.unique(joint_signal_unique_rows, return_counts=True)
  else: # Fallback for object dtype (less common here)
      _, counts = np.unique(joint_signal, axis=0, return_counts=True)

  # Calculate joint probability from counts
  joint_prob = counts / np.sum(counts)
  joint_prob = joint_prob[joint_prob > 0] # Only consider observed combinations

  joint_entropy = entropy(joint_prob)

  # Calculate sum of marginal entropies
  sum_marginal_entropies = np.sum(marginal_entropies)

  # Calculate mutual information
  mi = sum_marginal_entropies - joint_entropy

  # Normalize mutual information by the sum of marginal entropies
  # This is a common normalization method.
  # Ensure normalization factor is not zero
  normalization_factor = sum_marginal_entropies if sum_marginal_entropies > 1e-9 else 1.0

  # Mutual Information should be non-negative. Due to numerical precision, it might be slightly negative.
  return max(0, mi / normalization_factor)

"""You can then exploit the method to obtain the normalized mutual information associated with both the observations and the source estimations."""

mi_before = get_normalized_mi(observed_channels)
mi_after = get_normalized_mi(estimated_source_channels)

print(f'Normalized mutual information before ICA: {mi_before:.3f}')
print(f'Normalized mutual information after ICA: {mi_after:.3f}')

"""**HOW TO COMPLETE THE LAB**

To complete the laboratory, to first need to complete the method for computing the normalized mutual information, and then build an ad hoc pipeline to compute the optimal number of sources associated with a given EEG signal. To find the optimal source number, you may try all the possible values (up to the number of observations) and see what minimizes the normalized mutual information. Another option is to modify the pre-processing pipeline and consider only the whitened signals that ensure to maintain 95% of the **cumulative explained variance** (given by the PCA).

In the following, you are asked to implement the ICA pipeline (and your method to determine the optimal number of sources) to all the signals in the laboratory folder, considering **ALL** the patients and not only the one considered as a reference. Hence, for each signal, you have to assess the **performance of the FastICA algorithm** by computing the **normalized mutual information** associated with the **optimal number of signal sources**.

The results of the laboratory, including the mutual information and the optimal source number associated with each EEG signal, must be included in a report. In the report, you must describe the algorithm you implemented to estimate the optimal number of sources and discuss the limits and benefits of the proposed approach. You can also analyze an example of how your code operates over one of the EEG signals in the folder, reporting the signal before and after the ICA implementation.
"""

## First Trial

def preprocess_pca(observations: np.ndarray, variance_threshold: float = 0.95):

    # Center the observations
    observations, observation_means = center(observations)

    # Decorrelate and get eigenvalues/eigenvectors (PCA)
    # The output of decorrelate is already the PCA transformed data scaled by sqrt(eigenvalues)
    # and rotated by eigenvectors.
    whitened_signals_initial, eigenvalues, eigenvectors = decorrelate(observations, observation_means)

    # Sort eigenvalues and corresponding eigenvectors in descending order
    sorted_indices = np.argsort(eigenvalues)[::-1]
    sorted_eigenvalues = eigenvalues[sorted_indices]
    sorted_eigenvectors = eigenvectors[:, sorted_indices]

    # Calculate cumulative explained variance
    explained_variance_ratio = sorted_eigenvalues / np.sum(sorted_eigenvalues)
    cumulative_explained_variance = np.cumsum(explained_variance_ratio)

    # Find the number of components that explain the desired variance
    num_components = np.argmax(cumulative_explained_variance >= variance_threshold) + 1
    print(f"Retaining {num_components} components to explain >= {variance_threshold*100:.2f}% variance.")

    # Select the top principal components
    selected_eigenvectors = sorted_eigenvectors[:, :num_components]

    # Project the centered data onto the selected principal components and scale
    # We need to re-center the original data for this projection, as decorrelate modifies it.
    centered_observations = observations - observation_means # Use the centered data again
    projected_data = np.dot(selected_eigenvectors.T, centered_observations)

    # Scale the projected data using the selected eigenvalues
    # We scale by the inverse square root of the selected eigenvalues to whiten
    scaled_projected_data = np.dot(np.diag(np.power(sorted_eigenvalues[:num_components] + 1e-8, -1/2)), projected_data)

    return scaled_projected_data, observation_means, sorted_eigenvalues, sorted_eigenvectors, num_components


def find_optimal_source_number(observations: np.ndarray, variance_threshold: float = 0.95):

    # Preprocess with PCA dimensionality reduction based on cumulative variance
    whitened_channels_pca, _, _, _, optimal_source_num = preprocess_pca(observations, variance_threshold)

    # Apply FastICA using the determined number of sources
    if optimal_source_num == 0:
        print("No components retained based on variance threshold. Cannot perform ICA.")
        return 0, 1.0, np.array([]), whitened_channels_pca # Return default values or handle appropriately

    W = fast_ica(whitened_channels_pca, optimal_source_num, convergence_threshold=1e-10, max_iteration=500)

    # Estimate source channels
    estimated_source_channels = np.dot(W, whitened_channels_pca)

    # Calculate normalized mutual information of estimated sources
    mi_after_ica = get_normalized_mi(estimated_source_channels)

    return optimal_source_num, mi_after_ica, estimated_source_channels, whitened_channels_pca

# Example usage with EEG data:
optimal_source_number, mi_after_ica_eeg, estimated_source_channels_eeg, whitened_channels_pca_eeg = find_optimal_source_number(observed_channels, variance_threshold=0.95)

print(f'\nOptimal number of sources based on PCA variance threshold: {optimal_source_number}')
print(f'Normalized mutual information after ICA with optimal sources: {mi_after_ica_eeg:.3f}')

# Plotting the whitened channels after PCA reduction and the estimated source channels
if optimal_source_number > 0:
    plot_signal_channels(times=observed_times[:whitened_channels_pca_eeg.shape[1]], # Ensure time axis matches signal length
                         signal_channels=whitened_channels_pca_eeg,
                         channel_names=[f'PCA Comp {i+1}' for i in range(whitened_channels_pca_eeg.shape[0])],
                         amplitude=np.max(np.abs(whitened_channels_pca_eeg)),
                         title=f'Whitened Channels after PCA ({optimal_source_number} Components)')

    plot_signal_channels(times=observed_times[:estimated_source_channels_eeg.shape[1]], # Ensure time axis matches signal length
                         signal_channels=estimated_source_channels_eeg,
                         channel_names=[f'Source {i+1}' for i in range(estimated_source_channels_eeg.shape[0])],
                         amplitude=np.max(np.abs(estimated_source_channels_eeg)),
                         title=f'Estimated Source Channels ({optimal_source_number} Sources)')


results = []
for patient_folder in os.listdir(path_to_dataset_folder):
    patient_path = os.path.join(path_to_dataset_folder, patient_folder)
    if os.path.isdir(patient_path):
        for file_name in os.listdir(patient_path):
            if file_name.endswith('.edf'):
                file_path = os.path.join(patient_path, file_name)
                print(f"\nProcessing file: {file_name}")
                try:
                    raw_data_file = mne.io.read_raw_edf(file_path, preload=True, verbose=False) # preload=True for faster processing
                    signal_values_file = raw_data_file.get_data() * 1000


                    optimal_num, mi_after, _, _ = find_optimal_source_number(signal_values_file, variance_threshold=0.95)
                    results.append({'file': file_name, 'patient': patient_folder, 'optimal_sources': optimal_num, 'mi_after_ica': mi_after})
                except Exception as e:
                    print(f"Could not process file {file_name}: {e}")
                    results.append({'file': file_name, 'patient': patient_folder, 'optimal_sources': 'Error', 'mi_after_ica': 'Error', 'error': str(e)})


results_df = pd.DataFrame(results)
print("\nSummary of Results:")
print(results_df)

## after the modifications which involve adding the nmi before ica and also the visualization

results = []

if 'path_to_dataset_folder' not in locals():
    print("WARNING: 'path_to_dataset_folder' is not defined. Please define it before running the loop.")
    print("Example: path_to_dataset_folder = '/path/to/your/EEG_data_folder'")
else:
    if os.path.exists(path_to_dataset_folder):
        for patient_folder in os.listdir(path_to_dataset_folder):
            patient_path = os.path.join(path_to_dataset_folder, patient_folder)
            if os.path.isdir(patient_path):
                for file_name in os.listdir(patient_path):
                    if file_name.endswith('.edf'):
                        file_path = os.path.join(patient_path, file_name)
                        print(f"\nProcessing file: {file_name}")
                        try:
                            # Load data for the current file
                            raw_data_file = mne.io.read_raw_edf(file_path, preload=True, verbose=False)
                            current_observed_channels = raw_data_file.get_data() * 1000 # Convert to microvolts if needed
                            current_observed_times = raw_data_file.times # Get time information for plotting

                            optimal_num, mi_before, mi_after, estimated_sources_current, whitened_channels_current, original_data_current = \
                                find_optimal_source_number(current_observed_channels, variance_threshold=0.95)

                            results.append({
                                'file': file_name,
                                'patient': patient_folder,
                                'optimal_sources': optimal_num,
                                'mi_before_ica': mi_before,
                                'mi_after_ica': mi_after
                            })

                            print(f'Normalized mutual information BEFORE ICA: {mi_before:.3f}')
                            print(f'Normalized mutual information AFTER ICA with optimal sources: {mi_after:.3f}')

                            # Plotting for each file (Original, Whitened, Estimated Sources)
                            if optimal_num > 0:
                                # Original Signal Plot
                                plot_signal_channels(times=current_observed_times,
                                                     signal_channels=original_data_current,
                                                     channel_names=[f'Ch {i+1}' for i in range(original_data_current.shape[0])],
                                                     amplitude=np.max(np.abs(original_data_current)),
                                                     title=f'Original Signal: {file_name}')

                                # Whitened Signal Plot (after PCA reduction)
                                plot_signal_channels(times=current_observed_times[:whitened_channels_current.shape[1]],
                                                     signal_channels=whitened_channels_current,
                                                     channel_names=[f'PCA Comp {i+1}' for i in range(whitened_channels_current.shape[0])],
                                                     amplitude=np.max(np.abs(whitened_channels_current)),
                                                     title=f'Whitened Channels after PCA ({optimal_num} Components): {file_name}')

                                # Estimated Source Signal Plot
                                plot_signal_channels(times=current_observed_times[:estimated_sources_current.shape[1]],
                                                     signal_channels=estimated_sources_current,
                                                     channel_names=[f'Source {i+1}' for i in range(estimated_sources_current.shape[0])],
                                                     amplitude=np.max(np.abs(estimated_sources_current)),
                                                     title=f'Estimated Source Channels ({optimal_num} Sources): {file_name}')
                            else:
                                print(f"Skipping plots for {file_name} as optimal_num is 0.")


                        except Exception as e:
                            print(f"ERROR: Could not process file {file_name}: {e}")
                            results.append({'file': file_name, 'patient': patient_folder, 'optimal_sources': 'Error',
                                            'mi_before_ica': 'Error', 'mi_after_ica': 'Error', 'error': str(e)})
    else:
        print(f"Error: Dataset path not found at '{path_to_dataset_folder}'. Please set 'path_to_dataset_folder' correctly.")

results_df = pd.DataFrame(results)
print("\nSummary of All Processed Results:")
print(results_df)

## the following code must be the same as previous block just with a better visualization of the original signal

def plot_signal_channels(times, signal_channels, channel_names, amplitude, title, fig_height_multiplier=0.3):

    n_channels = signal_channels.shape[0]
    fig_height = n_channels * fig_height_multiplier # Dynamically adjust figure height
    plt.figure(figsize=(15, fig_height)) # Wider figure for better x-axis readability

    max_amp = np.max(np.abs(signal_channels))

    plot_amplitude = max_amp * 2.5 # A multiplier to ensure good separation, adjust as needed

    for i in range(n_channels):
        # Offset each channel vertically for clarity
        plt.plot(times, signal_channels[i] + i * plot_amplitude, 'k-', linewidth=0.7) # Black line for signals

    plt.title(title, fontsize=16)
    plt.xlabel('Time [s]', fontsize=14)
    plt.ylabel('Channel', fontsize=14)

    # Set Y-axis ticks to display channel names
    plt.yticks(np.arange(n_channels) * plot_amplitude, channel_names, fontsize=10) # Smaller font for many labels
    plt.grid(True, linestyle=':', alpha=0.7)
    plt.tight_layout() # Adjust layout to prevent labels from overlapping
    plt.show()

def preprocess_pca(observations: np.ndarray, variance_threshold: float = 0.95):

    # Center the observations
    centered_original_observations, observation_means = center(observations.copy()) # .copy() to preserve original observations

    # Decorrelate and get eigenvalues/eigenvectors (PCA)
    whitened_signals_initial, eigenvalues, eigenvectors = decorrelate(observations, observation_means)

    # Sort eigenvalues and corresponding eigenvectors in descending order
    sorted_indices = np.argsort(eigenvalues)[::-1]
    sorted_eigenvalues = eigenvalues[sorted_indices]
    sorted_eigenvectors = eigenvectors[:, sorted_indices]

    # Calculate cumulative explained variance
    explained_variance_ratio = sorted_eigenvalues / np.sum(sorted_eigenvalues)
    cumulative_explained_variance = np.cumsum(explained_variance_ratio)

    # Find the number of components that explain the desired variance
    num_components = np.argmax(cumulative_explained_variance >= variance_threshold) + 1
    print(f"Retaining {num_components} components to explain >= {variance_threshold*100:.2f}% variance.")

    # Select the top principal components
    selected_eigenvectors = sorted_eigenvectors[:, :num_components]

    # Project the centered data onto the selected principal components and scale
    projected_data = np.dot(selected_eigenvectors.T, centered_original_observations)

    # Scale the projected data using the selected eigenvalues
    scaled_projected_data = np.dot(np.diag(np.power(sorted_eigenvalues[:num_components] + 1e-8, -1/2)), projected_data)

    return scaled_projected_data, observation_means, sorted_eigenvalues, sorted_eigenvectors, num_components, centered_original_observations


def find_optimal_source_number(observations: np.ndarray, original_channel_names: list, variance_threshold: float = 0.95):

    # Preprocess with PCA dimensionality reduction based on cumulative variance
    whitened_channels_pca, _, _, _, optimal_source_num, centered_original_observations = preprocess_pca(observations, variance_threshold)

    # Calculate NMI before ICA (using the centered original observations)
    mi_before_ica = get_normalized_mi(centered_original_observations)

    # Apply FastICA using the determined number of sources
    if optimal_source_num == 0:
        print("No components retained based on variance threshold. Cannot perform ICA.")
        return 0, mi_before_ica, 1.0, np.array([]), whitened_channels_pca, observations, original_channel_names # Return default values or handle appropriately

    W = fast_ica(whitened_channels_pca, optimal_source_num, convergence_threshold=1e-10, max_iteration=500)

    # Estimate source channels
    estimated_source_channels = np.dot(W, whitened_channels_pca)

    # Calculate normalized mutual information of estimated sources
    mi_after_ica = get_normalized_mi(estimated_source_channels)

    return optimal_source_num, mi_before_ica, mi_after_ica, estimated_source_channels, whitened_channels_pca, observations, original_channel_names

path_to_dataset_folder = './ehealth_lab_3/' # <-- SET YOUR ACTUAL DATASET PATH HERE

results = []

if os.path.exists(path_to_dataset_folder):
    for patient_folder in os.listdir(path_to_dataset_folder):
        patient_path = os.path.join(path_to_dataset_folder, patient_folder)
        if os.path.isdir(patient_path):
            for file_name in os.listdir(patient_path):
                if file_name.endswith('.edf'):
                    file_path = os.path.join(patient_path, file_name)
                    print(f"\nProcessing file: {file_name}")
                    try:
                        raw_data_file = mne.io.read_raw_edf(file_path, preload=True, verbose=False)
                        current_observed_channels = raw_data_file.get_data() * 1000 # Convert to microvolts
                        current_observed_times = raw_data_file.times
                        current_channel_names = raw_data_file.ch_names # Get actual channel names from MNE

                        optimal_num, mi_before, mi_after, estimated_sources_current, whitened_channels_current, original_data_current, original_ch_names_for_plot = \
                            find_optimal_source_number(current_observed_channels, current_channel_names, variance_threshold=0.95)

                        results.append({
                            'file': file_name,
                            'patient': patient_folder,
                            'optimal_sources': optimal_num,
                            'mi_before_ica': mi_before,
                            'mi_after_ica': mi_after
                        })

                        print(f'Normalized mutual information BEFORE ICA: {mi_before:.3f}')
                        print(f'Normalized mutual information AFTER ICA with optimal sources: {mi_after:.3f}')

                        # Plotting for each file (Original, Whitened, Estimated Sources)
                        if optimal_num > 0:
                            # Original Signal Plot - uses actual channel names and adjusted plot_amplitude
                            plot_signal_channels(times=current_observed_times,
                                                 signal_channels=original_data_current,
                                                 channel_names=original_ch_names_for_plot, # Use actual names here
                                                 amplitude=np.max(np.abs(original_data_current)), # Original amplitude for scaling
                                                 title=f'Original Signal: {file_name}',
                                                 fig_height_multiplier=0.3 # Adjust this for more/less vertical space
                                                )

                            # Whitened Signal Plot (after PCA reduction) - channel names will be 'PCA Comp X'
                            plot_signal_channels(times=current_observed_times[:whitened_channels_current.shape[1]],
                                                 signal_channels=whitened_channels_current,
                                                 channel_names=[f'PCA Comp {i+1}' for i in range(whitened_channels_current.shape[0])],
                                                 amplitude=np.max(np.abs(whitened_channels_current)),
                                                 title=f'Whitened Channels after PCA ({optimal_num} Components): {file_name}',
                                                 fig_height_multiplier=0.8 # PCA plots might need more space per channel
                                                )

                            # Estimated Source Signal Plot - channel names will be 'Source X'
                            plot_signal_channels(times=current_observed_times[:estimated_sources_current.shape[1]],
                                                 signal_channels=estimated_sources_current,
                                                 channel_names=[f'Source {i+1}' for i in range(estimated_sources_current.shape[0])],
                                                 amplitude=np.max(np.abs(estimated_sources_current)),
                                                 title=f'Estimated Source Channels ({optimal_num} Sources): {file_name}',
                                                 fig_height_multiplier=0.8 # Source plots might need more space per channel
                                                )
                        else:
                            print(f"Skipping plots for {file_name} as optimal_num is 0.")

                    except Exception as e:
                        print(f"ERROR: Could not process file {file_name}: {e}")
                        results.append({'file': file_name, 'patient': patient_folder, 'optimal_sources': 'Error',
                                        'mi_before_ica': 'Error', 'mi_after_ica': 'Error', 'error': str(e)})
else:
    print(f"Error: Dataset path not found at '{path_to_dataset_folder}'. Please set 'path_to_dataset_folder' correctly.")

results_df = pd.DataFrame(results)
print("\nSummary of All Processed Results:")
print(results_df)











